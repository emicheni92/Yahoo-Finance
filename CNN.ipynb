{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589a8ba3-cb07-4cb5-bb02-27b1c06c7cb0",
   "metadata": {},
   "source": [
    "Step 1: Add libraries such as TensorFlow.\n",
    "TensorFlow will be our main framework for developing and optimizing the autoencoder. Among the other tools we’ll be utilizing for data processing and visualization are pandas, matplotlib, and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06725dd-c0ed-4f78-a387-d780a329ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feda9dd-34fd-4f40-b458-562d2a9b0490",
   "metadata": {},
   "source": [
    "Step 2: Load and prepare the dataset\n",
    "TensorFlow provides the Fashion MNIST dataset, which we will utilize. The training and testing sets will be loaded and the pixel values will be normalized to the interval [-1, 1]. Since our autoencoder uses a single neuron for the output layer. We will also restructure the pictures to have a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d0bc-721a-4112-8db8-37dbccfab102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train = x_train.astype('float32') / 255.0 * 2 - 1\n",
    "x_test = x_test.astype('float32') / 255.0 * 2 - 1\n",
    "\n",
    "# Reshape the images to have a single channel\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5666cc3-4112-4780-91f7-eb45ac02ff2e",
   "metadata": {},
   "source": [
    "Step 3: Define the model architecture and parameters\n",
    "Two thick layers—one for the encoder and one for the decoder—will be used in our straightforward autoencoder. 64 neurons make up the encoder, which maps the input to a 64-dimensional latent space using a sigmoid activation function. In order to map the latent space back to the original picture space of 784 dimensions (28×28 pixels). The decoder will employ a tan h activation function with 784 neurons.\n",
    "\n",
    "Model definition will be done via the Keras Sequential API and model compilation will be done using an Adam optimizer and a mean squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32b9f8-5202-4000-8967-f99ae6c20658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "  # Encoder layer\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  # Decoder layer\n",
    "  tf.keras.layers.Dense(784, activation='tanh'),\n",
    "  tf.keras.layers.Reshape((28, 28, 1))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb6748-e2e6-4d39-869c-d612d581fcbe",
   "metadata": {},
   "source": [
    "Step 4: Train and evaluate the model\n",
    "Using a batch size of 256 and a validation split of 0.2. We will train the model across ten epochs. After every epoch, the model weights will be saved using a callback method. In order to assess the model we will compute the reconstruction error on the test set and display a few of the original and rebuilt photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea4908-e510-4ae3-b752-06ed895c08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function to save the model weights\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model.save_weights(f'autoencoder_{epoch+1}.weights.h5')\n",
    "        print(f'Saved model weights for epoch {epoch+1}')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, x_train, epochs=10, batch_size=256, validation_split=0.2, callbacks=[SaveModel()])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(x_test, x_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Visualize some of the original and reconstructed images\n",
    "n = 10  # Number of images to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstructed image\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    reconstructed = model.predict(x_test[i].reshape(1, 28, 28, 1))\n",
    "    plt.imshow(reconstructed.reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533d962-5451-4a6f-943f-74d2e600e792",
   "metadata": {},
   "source": [
    "Building Image denoising\n",
    "The process of removing noise from a picture, which might include many kinds of such speckle, salt-and-pepper, and Gaussian noise, is known as image denoising. Image denoising can enhance an image clarity and quality or get it ready for other uses, such segmentation or classification.\n",
    "\n",
    "We will use a neural autoencoder to denoise the same MNIST dataset in this case. The autoencoder will be taught to separate the clean pictures from the noisy ones once the original photographs are exposed to Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ee17e-5a1c-4d09-bb29-873801283c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde33360-79c3-4867-87c9-85d08fdb98f1",
   "metadata": {},
   "source": [
    "Step 2: Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758bbb3-afcf-49ad-affd-58d4a6989c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the images to have a single channel\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Add Gaussian noise to the images\n",
    "noise_factor = 0.1\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "# Clip the noisy images to the range [0, 1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abd8ce-bdd5-41f9-bca2-437547298a5d",
   "metadata": {},
   "source": [
    "Step 3: Define the model architecture and parameters\n",
    "Four convolutional layers—two for the encoder and two for the decoder—will be used in our convolutional autoencoder. By using 32 and 16 filters, together with a kernel size of 3 and a stride of 2. The encoder will cut the input’s spatial dimensions in half. The input spatial dimensions will be increased by half thanks to the decoder employment of 16 and 1 filters, a kernel size of 3, and a stride of 2. A sigmoid activation function will also be used by the decoder to map the output to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2cc8c-5126-4878-9695-179189ed93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "def encoder(inputs):\n",
    "  # Convolutional layer with 32 filters, 3x3 kernel, and 2x2 stride\n",
    "  x = tf.keras.layers.Conv2D(32, 3, 2, padding='same')(inputs)\n",
    "  # Convolutional layer with 16 filters, 3x3 kernel, and 2x2 stride\n",
    "  x = tf.keras.layers.Conv2D(16, 3, 2, padding='same')(x)\n",
    "  return x\n",
    "\n",
    "# Define the decoder\n",
    "def decoder(inputs):\n",
    "  # Convolutional transpose layer with 16 filters, 3x3 kernel, and 2x2 stride\n",
    "  x = tf.keras.layers.Conv2DTranspose(16, 3, 2, padding='same')(inputs)\n",
    "  # Convolutional transpose layer with 1 filter, 3x3 kernel, and 2x2 stride\n",
    "  x = tf.keras.layers.Conv2DTranspose(1, 3, 2, padding='same', activation='sigmoid')(x)\n",
    "  return x\n",
    "\n",
    "# Define the input layer\n",
    "inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=decoder(encoder(inputs)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71934a-4493-4824-bb37-2699a3559927",
   "metadata": {},
   "source": [
    "Step 4: Train and evaluate the model\n",
    "Using a batch size of 128 and a validation split of 0.2. We will train the model for 50 epochs. After every epoch, the model weights will be saved using a callback method. In order to assess the model , we will compute the reconstruction error on the test set and visualize a portion of the noisy, original photos that have been denoised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad43c5-e7cc-458d-84f3-aec7cfa19057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function to save the model weights\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model.save_weights(f'denoising_{epoch+1}.weights.h5')\n",
    "        print(f'Saved model weights for epoch {epoch+1}')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, validation_split=0.2, callbacks=[SaveModel()])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(x_test_noisy, x_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Visualize some of the original and reconstructed images\n",
    "n = 10  # Number of images to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstructed image\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    reconstructed = model.predict(x_test[i].reshape(1, 28, 28, 1))\n",
    "    plt.imshow(reconstructed.reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f7490-81f2-426c-bf20-171e5f8f6f4d",
   "metadata": {},
   "source": [
    "conclusion\n",
    "We have learnt how to utilize TensorFlow for generative modeling in this article. We have also seen two example code with step by step implementation using TensorFlow for generative modeling. This is just a glimpse into the exciting world of generative modeling with TensorFlow. With practice and exploration, you can use this powerful tool to bring your creative ideas to life from generating futuristic landscapes to composing new musical styles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
